---
title: Architecture
description: rtdl architecture
---

# Architecture üèõ
rtdl has a multi-service architecture composed of tested and trusted open source tools 
to process and catalog your data and custom-built services to interact with them more easily.

## config services

### config
API service written in Go. Use the API to create, read, update, activate, deactivate, 
and delete `stream` records. `stream` records store the configuration information for 
the different data streams you want to send to your data lake. This service can also be 
used to lookup master data necessary for creating successful `stream` records like 
`file_store_types`, `partition_times`, and `compression_types`.  
**Environment Variables:** RTDL_DB_HOST, RTDL_DB_USER, RTDL_DB_PASSWORD, RTDL_DB_DBNAME  
**Public Port:** 80  
**Endpoints:**
  * /getStream -- POST; `stream_id` required
  * /getAllStreams -- GET
  * /getAllActiveStreams -- GET
  * /createStream -- POST; `message_type` and `folder_name` required
  * /updateStream -- PUT; all fields required (any missing fields will be replaced with NULL 
    values)
  * /deleteStream -- DELETE; `stream_id` required
  * /activateStream -- PUT; `stream_id` required
  * /deactivateStream -- PUT; `stream_id` required
  * /getAllFileStoreTypes -- GET
  * /getAllPartitionTimes -- GET
  * /getAllCompressionTypes -- GET
  
  Sample ```createStream``` payload for creating Parquet file in AWS S3
  ```	
  {
	"active": true,
  "message_type": "test-msg-aws",
	"file_store_type_id": 2,
	"region": "us-east-1",
	"bucket_name": "testBucketAWS",
	"folder_name": "testFolderAWS",
  "partition_time_id": 1,
  "compression_type_id": 1,
	"aws_access_key_id": "[aws_access_key_id]",
  "aws_secret_access_key": "[aws_secret_access_key]"
  }
  ```
  
  ```file_store_type_id``` - 1 for Local, 2 for AWS, 3 for GCS
  ```partiion_time_id``` - 1 - HOURLY, 2 - DAILY, 3 - WEEKLY, 4 - MONTHLY, 5 - QAURTERLY
  
  For cloud storage - final file path would be 
  ```<bucket>/<folder>/<message type>/<time partition>/*.parquet```
  
  ```time partition``` part can look like 
	```2021-06-15-13```(Hourly), 
	```2021-06-15```(Daily),
	```2021-48```(Weekly - ISOWeek), 
	```2021-06```(Monthly),
	```2021-02```(Quarterly)
	
  The leaf-level file would have timestamp upto milliseconds as the file name
  

### rtdl-db
YugabyteDB or PostgreSQL (both configurations included in the docker compose files). This service 
stores the `stream` configuration data written by the `config` service and read by the `ingester` 
stateful function  
  * **Database Name:** rtdl_db
  * **Username:** rtdl
  * **Password:** rtdl

**Tables**
  * file_store_types
    * file_store_type_id SERIAL,
    * file_store_type_name VARCHAR,
    * PRIMARY KEY (file_store_type_id)
  * partition_times
    * partition_time_id SERIAL,
    * partition_time_name VARCHAR,
    * PRIMARY KEY (partition_time_id)
  * compression_types
    * compression_type_id SERIAL,
    * compression_type_name VARCHAR,
    * PRIMARY KEY (compression_type_id)
  * streams
    * stream_id uuid DEFAULT gen_random_uuid(),
    * stream_alt_id VARCHAR,
    * active BOOLEAN DEFAULT FALSE,
    * message_type VARCHAR NOT NULL,
    * file_store_type_id INTEGER DEFAULT 1,
    * region VARCHAR,
    * bucket_name VARCHAR,
    * folder_name VARCHAR NOT NULL,
    * partition_time_id INTEGER DEFAULT 1,
    * compression_type_id INTEGER DEFAULT 1,
    * aws_access_key_id VARCHAR,
    * aws_secret_access_key VARCHAR,
    * gcp_json_credentials VARCHAR,
    * created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    * updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    * PRIMARY KEY (stream_id),
    * FOREIGN KEY(file_store_type_id) REFERENCES file_store_types(file_store_type_id),
    * FOREIGN KEY(partition_time_id) REFERENCES partition_times(partition_time_id),
    * FOREIGN KEY(compression_type_id) REFERENCES compression_types(compression_type_id)

## ingest service
Service written in Go that accepts a JSON payload and writes it to Kafka for processing by the 
`ingester` stateful function. 
**Public Port:** 8080  
**Endpoints:**
  * /ingest -- POST; accepts JSON payload along with a write key
  * /refreshCache -- GET; triggers a refresh of the streams cache in the `ingester` stateful function

## kafka services
Standard Kafka services. Creates data streams that can be read by a Stateful Function. Images from Bitnami.
  * kafka-zookeeper - Apache Zookeeper service
  * kafka - Apache Kafka service
    **Public Port:** 9092

## process services
Apache Flink [Stateful Functions](https://flink.apache.org/stateful-functions.html) cluster in a standard 
configuration ‚Äì a job manager service with paired task manager and stateful function services.
  * statefun-manager - Apache Flink Stateful Functions manager service  
  * statefun-worker - Apache Flink Stateful Functions task manager service
  * statefun-functions - Apache Flink Stateful function written in Go named `ingester`. Reads JSON 
    payloads posted to Kafka, processes and stores the data in Parque format based on the configuration 
    in the associated streams record.  
    **Environment Variables:** RTDL_DB_HOST, RTDL_DB_USER, RTDL_DB_PASSWORD, RTDL_DB_DBNAME, DREMIO_HOST, 
    DREMIO_PORT, DREMIO_USERNAME, DREMIO_PASSWORD, DREMIO_MOUNT_PATH


## dremio service
Standard Dremio service. Dremio makes the data in your data lake accessible in real-time. Dremio makes it easy 
to discover, curate, accelerate, and share data. Use port 9047 to access Dremio's UI and query your data.
**Public Ports:** 9047, 31010, 45678  
  * dremio - Dremio's [dremio-oss](https://github.com/dremio/dremio-oss) service
  